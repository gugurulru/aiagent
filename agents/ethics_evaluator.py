# agents/ethics_evaluator.py

from typing import Dict, List
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

from state_schema import PipelineState, EthicsEvidence, EthicsCategoryEvaluation


class EthicsEvaluator:
    """
    ìƒì„±ëœ í‰ê°€ ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ í‰ê°€ ìˆ˜í–‰ Agent
    
    í”„ë¡œì„¸ìŠ¤:
    1. Stateì—ì„œ í‰ê°€ ê¸°ì¤€ ë¡œë“œ (ethics_evaluation_criteria)
    2. Stateì—ì„œ ìˆ˜ì§‘ëœ ë¬¸ì„œ ë¡œë“œ (web_collection + specialized_collection)
    3. ê° ê¸°ì¤€ë³„ë¡œ ë¬¸ì„œ ë¶„ì„ ë° ì ìˆ˜ ë¶€ì—¬
    4. ìƒì„¸í•œ ê·¼ê±°ì™€ í•¨ê»˜ í‰ê°€ ê²°ê³¼ ì €ì¥
    """
    
    # í‰ê°€ ì¹´í…Œê³ ë¦¬
    CATEGORIES = [
        "transparency",
        "human_oversight", 
        "data_governance",
        "accuracy_validation",
        "accountability"
    ]
    
    # ì¬ìˆ˜ì§‘ ì„ê³„ê°’
    MIN_CONFIDENCE_THRESHOLD = 0.6
    MIN_EVIDENCE_COUNT = 3
    
    def __init__(self, openai_api_key: str):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            api_key=openai_api_key,
            temperature=0
        )
    
    def execute(self, state: PipelineState) -> PipelineState:
        """ë©”ì¸ ì‹¤í–‰: í‰ê°€ ê¸°ì¤€ ê¸°ë°˜ ì‹¤ì œ í‰ê°€"""
        print("\n" + "="*70)
        print("âš–ï¸ [ìœ¤ë¦¬ í‰ê°€ ì‹¤í–‰ Agent] ì‹œì‘")
        print("="*70)
        
        company_name = state["company_name"]
        domain = state["domain"]
        
        # 1. í‰ê°€ ê¸°ì¤€ í™•ì¸
        if "ethics_evaluation_criteria" not in state:
            print("âŒ í‰ê°€ ê¸°ì¤€ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € EthicsCriteriaGeneratorë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            state["errors"].append({
                "stage": "ethics_evaluation",
                "error": "í‰ê°€ ê¸°ì¤€ ì—†ìŒ",
                "timestamp": datetime.now().isoformat(),
                "recovered": False,
                "traceback": None,
                "impact_on_reliability": "high"
            })
            return state
        
        criteria = state["ethics_evaluation_criteria"]
        print(f"âœ… í‰ê°€ ê¸°ì¤€ ë¡œë“œ: {criteria['total_criteria_count']}ê°œ")
        
        # 2. ìˆ˜ì§‘ëœ ë¬¸ì„œ í™•ì¸
        web_docs = state.get("web_collection", {}).get("documents", [])
        spec_docs = state.get("specialized_collection", {}).get("documents", [])
        all_docs = web_docs + spec_docs
        
        print(f"ğŸ“š í‰ê°€ ëŒ€ìƒ ë¬¸ì„œ:")
        print(f"   - ì›¹ ìˆ˜ì§‘: {len(web_docs)}ê°œ")
        print(f"   - ì „ë¬¸ ìë£Œ: {len(spec_docs)}ê°œ")
        print(f"   - ì´: {len(all_docs)}ê°œ")
        
        if len(all_docs) == 0:
            print("âŒ í‰ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            state["errors"].append({
                "stage": "ethics_evaluation",
                "error": "í‰ê°€ ë¬¸ì„œ ì—†ìŒ",
                "timestamp": datetime.now().isoformat(),
                "recovered": False,
                "traceback": None,
                "impact_on_reliability": "high"
            })
            return state
        
        try:
            # 3. ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ ìˆ˜í–‰
            evaluations = {}
            all_evidence = []
            
            for category in self.CATEGORIES:
                print(f"\n{'='*70}")
                print(f"ğŸ“‹ {category.upper()} í‰ê°€ ì¤‘...")
                print(f"{'='*70}")
                
                # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í‰ê°€ ê¸°ì¤€ ê°€ì ¸ì˜¤ê¸°
                category_criteria = criteria['categories'].get(category, {})
                
                if not category_criteria or not category_criteria.get('criteria'):
                    print(f"âš ï¸ {category} í‰ê°€ ê¸°ì¤€ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                    continue
                
                print(f"ğŸ“ í‰ê°€ ê¸°ì¤€: {len(category_criteria['criteria'])}ê°œ")
                
                # í‰ê°€ ìˆ˜í–‰
                evaluation = self._evaluate_category(
                    category=category,
                    company_name=company_name,
                    documents=all_docs,
                    criteria=category_criteria
                )
                
                evaluations[category] = evaluation
                all_evidence.extend(evaluation["evidence"])
                
                print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼:")
                print(f"   ì ìˆ˜: {evaluation['score']}/100")
                print(f"   ì‹ ë¢°ë„: {evaluation['confidence']:.2f}")
                print(f"   ê·¼ê±°: {len(evaluation['evidence'])}ê°œ")
                print(f"   ì´ìŠˆ: {len(evaluation['issues'])}ê°œ")
                print(f"   ê°•ì : {len(evaluation['strengths'])}ê°œ")
            
            # 4. ì ìˆ˜ ê³„ì‚°
            ethics_score = self._calculate_scores(evaluations)
            
            # 5. Critical Issues ì¶”ì¶œ
            critical_issues = self._identify_critical_issues(evaluations)
            
            # 6. State ì—…ë°ì´íŠ¸
            state["ethics_evaluation"] = {
                "transparency": evaluations.get("transparency", self._get_empty_evaluation()),
                "human_oversight": evaluations.get("human_oversight", self._get_empty_evaluation()),
                "data_governance": evaluations.get("data_governance", self._get_empty_evaluation()),
                "accuracy_validation": evaluations.get("accuracy_validation", self._get_empty_evaluation()),
                "accountability": evaluations.get("accountability", self._get_empty_evaluation()),
                "all_sources_used": list(set(e["source_document_id"] for e in all_evidence)),
                "total_evidence_count": len(all_evidence)
            }
            
            state["ethics_score"] = ethics_score
            state["critical_issues"] = critical_issues
            
            # 7. ì¬ìˆ˜ì§‘ í•„ìš” ì—¬ë¶€ íŒë‹¨
            needs_recollection = self._check_recollection_needed(
                ethics_score=ethics_score,
                evaluations=evaluations
            )
            
            if needs_recollection:
                print("\nâš ï¸ ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ì¬ìˆ˜ì§‘ í•„ìš”")
                state["is_data_sufficient"] = False
                state["retry_collection"] = state.get("retry_collection", 0) + 1
                state["collection_focus"] = self._suggest_collection_focus(evaluations)
            else:
                print("\nâœ… ìœ¤ë¦¬ í‰ê°€ ì™„ë£Œ")
                state["is_data_sufficient"] = True
            
            print(f"\n{'='*70}")
            print(f"ğŸ‰ ìœ¤ë¦¬ í‰ê°€ ì™„ë£Œ!")
            print(f"{'='*70}")
            print(f"ğŸ“Š ìµœì¢… ì ìˆ˜: {ethics_score['total']}/100 (ë“±ê¸‰: {ethics_score['grade']})")
            print(f"ğŸ’¡ ì „ì²´ ì‹ ë¢°ë„: {ethics_score['overall_confidence']:.2f}")
            print(f"âš ï¸ Critical Issues: {len(critical_issues)}ê°œ")
            print(f"ğŸ“ ì´ ê·¼ê±°: {len(all_evidence)}ê°œ")
            
        except Exception as e:
            print(f"âŒ ìœ¤ë¦¬ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            
            state["errors"].append({
                "stage": "ethics_evaluation",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "recovered": False,
                "traceback": traceback.format_exc(),
                "impact_on_reliability": "high"
            })
        
        return state
    
    def _evaluate_category(
        self,
        category: str,
        company_name: str,
        documents: List[Dict],
        criteria: Dict
    ) -> EthicsCategoryEvaluation:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ í‰ê°€ ìˆ˜í–‰"""
        
        # ë¬¸ì„œ ìš”ì•½ ì¤€ë¹„
        doc_summaries = []
        for i, doc in enumerate(documents[:50], 1):  # ìµœëŒ€ 50ê°œ
            summary = (
                f"[ë¬¸ì„œ {i}]\n"
                f"ID: {doc['id']}\n"
                f"ì œëª©: {doc['title']}\n"
                f"ì¶œì²˜: {doc['source_category']}\n"
                f"ë°œí–‰ì²˜: {doc.get('publisher', 'N/A')}\n"
                f"ì‹ ë¢°ë„: {doc['reliability']} ({doc['reliability_score']})\n"
                f"ë‚´ìš©: {doc.get('excerpt', doc.get('content', ''))[:300]}\n"
            )
            doc_summaries.append(summary)
        
        # í‰ê°€ ê¸°ì¤€ ì •ë¦¬
        criteria_list = []
        for i, crit in enumerate(criteria['criteria'], 1):
            criteria_text = (
                f"[ê¸°ì¤€ {i}] {crit['title']}\n"
                f"ID: {crit['id']}\n"
                f"ì„¤ëª…: {crit['description']}\n"
                f"ì¸¡ì • ë°©ë²•: {crit['measurement']}\n"
                f"ê°€ì¤‘ì¹˜: {crit['weight']}\n"
                f"í†µê³¼ ê¸°ì¤€: {crit['pass_threshold']}\n"
                f"ì¤‘ìš”ì„±: {crit['importance']}\n"
            )
            criteria_list.append(criteria_text)
        
        # í‰ê°€ í”„ë¡¬í”„íŠ¸
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ EU AI Act ì¤€ìˆ˜ í‰ê°€ìì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ í‰ê°€ ê¸°ì¤€ê³¼ íšŒì‚¬ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ {company_name}ì˜ {category} ì¤€ìˆ˜ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”.

í‰ê°€ ì§€ì¹¨:
1. ê° í‰ê°€ ê¸°ì¤€ì„ ê°œë³„ì ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”
2. íšŒì‚¬ ë¬¸ì„œì—ì„œ ê° ê¸°ì¤€ê³¼ ê´€ë ¨ëœ ì¦ê±°ë¥¼ ì°¾ìœ¼ì„¸ìš”
3. ì¦ê±°ì˜ ì§ˆê³¼ ì–‘ì— ë”°ë¼ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”
4. ì ìˆ˜ ì‚°ì •:
   - ê¸°ì¤€ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³ ë ¤í•˜ì„¸ìš”
   - ì§ì ‘ ì¦ê±°(ëª…ì‹œì  ì–¸ê¸‰): 100% ë°˜ì˜
   - ê°„ì ‘ ì¦ê±°(ì•”ì‹œì  ë‚´ìš©): 60% ë°˜ì˜
   - ì¦ê±° ì—†ìŒ: 0%
5. ë§¤ìš° êµ¬ì²´ì ìœ¼ë¡œ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”

ì ìˆ˜ ê³„ì‚° ë°©ë²•:
- ê° ê¸°ì¤€ë³„ë¡œ 0-100ì  ë¶€ì—¬
- ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ ì¢…í•© ì ìˆ˜ ê³„ì‚°
- ì˜ˆ: ê¸°ì¤€1(ê°€ì¤‘ì¹˜0.9, 80ì ) + ê¸°ì¤€2(ê°€ì¤‘ì¹˜0.7, 60ì ) + ...

JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
{{
    "score": 0-100,
    "level": 1-5,
    "confidence": 0.0-1.0,
    "evidence": [
        {{
            "finding": "ë§¤ìš° êµ¬ì²´ì ì¸ ë°œê²¬ì‚¬í•­ - ì–´ë–¤ ë¬¸ì„œì—ì„œ ì–´ë–¤ ë‚´ìš©ì„ ë°œê²¬í–ˆëŠ”ì§€",
            "source": "ë¬¸ì„œ ì œëª©",
            "url": "ë¬¸ì„œ URL (ìˆìœ¼ë©´)",
            "tier": "tier1/tier2/tier3",
            "weight": 0.0-1.0,
            "source_document_id": "ë¬¸ì„œ ID",
            "evidence_type": "direct/indirect/inferred",
            "reliability": "high/medium/low",
            "criterion_id": "í‰ê°€ ê¸°ì¤€ ID",
            "criterion_title": "í‰ê°€ ê¸°ì¤€ ì œëª©",
            "score_contribution": "ì´ ê·¼ê±°ê°€ ì „ì²´ ì ìˆ˜ì— ê¸°ì—¬í•œ ì •ë„ (0-100)",
            "detailed_explanation": "ì´ ê·¼ê±°ê°€ ì™œ ì¤‘ìš”í•œì§€, ì–´ë–»ê²Œ í‰ê°€ ê¸°ì¤€ì„ ì¶©ì¡±/ë¯¸ì¶©ì¡±í•˜ëŠ”ì§€ 300ì ì´ìƒ ìƒì„¸ ì„¤ëª…"
        }}
    ],
    "issues": ["êµ¬ì²´ì  ë¬¸ì œì  - ì–´ë–¤ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆëŠ”ì§€"],
    "strengths": ["êµ¬ì²´ì  ê°•ì  - ì–´ë–¤ ê¸°ì¤€ì„ ì˜ ì¶©ì¡±í–ˆëŠ”ì§€"],
    "information_availability": "abundant/sufficient/limited/none",
    "limitations": ["ë¶€ì¡±í•œ ì •ë³´ - ì–´ë–¤ ê¸°ì¤€ì„ í‰ê°€í•˜ê¸°ì— ì •ë³´ê°€ ë¶€ì¡±í•œì§€"],
    "criteria_scores": {{
        "criterion_id": {{
            "score": 0-100,
            "status": "pass/fail/partial",
            "evidence_count": ìˆ«ì
        }}
    }}
}}"""),
            ("user", """íšŒì‚¬: {company_name}
ì¹´í…Œê³ ë¦¬: {category}

=== í‰ê°€ ê¸°ì¤€ ===
{criteria}

=== íšŒì‚¬ ë¬¸ì„œ ===
{documents}

ìœ„ í‰ê°€ ê¸°ì¤€ì— ë”°ë¼ íšŒì‚¬ë¥¼ í‰ê°€í•˜ì„¸ìš”:""")
        ])
        
        chain = prompt | self.llm
        
        response = chain.invoke({
            "company_name": company_name,
            "category": category,
            "criteria": "\n\n".join(criteria_list),
            "documents": "\n\n".join(doc_summaries)
        })
        
        # JSON íŒŒì‹±
        try:
            import json
            import re
            
            json_match = re.search(r'```json\s*(.*?)\s*```', response.content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                result = json.loads(response.content)
            
            # í†µê³„ ê³„ì‚°
            result["evidence_count"] = len(result.get("evidence", []))
            result["tier1_evidence_count"] = sum(
                1 for e in result.get("evidence", []) 
                if e.get("tier") == "tier1"
            )
            result["direct_evidence_count"] = sum(
                1 for e in result.get("evidence", []) 
                if e.get("evidence_type") == "direct"
            )
            
            return result
            
        except Exception as e:
            print(f"    âš ï¸ í‰ê°€ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return self._get_empty_evaluation()
    
    def _calculate_scores(self, evaluations: Dict) -> Dict:
        """ì ìˆ˜ ê³„ì‚°"""
        scores = {}
        
        for cat in self.CATEGORIES:
            if cat in evaluations:
                scores[cat] = evaluations[cat]["score"]
            else:
                scores[cat] = 0
        
        # ì „ì²´ í‰ê· 
        valid_scores = [s for s in scores.values() if s > 0]
        total = sum(valid_scores) // len(valid_scores) if valid_scores else 0
        
        # ë“±ê¸‰
        if total >= 90:
            grade = "A+"
        elif total >= 80:
            grade = "A"
        elif total >= 70:
            grade = "B"
        elif total >= 60:
            grade = "C"
        else:
            grade = "D"
        
        # ì‹ ë¢°ë„
        confidences = {}
        for cat in self.CATEGORIES:
            if cat in evaluations:
                confidences[cat] = evaluations[cat]["confidence"]
            else:
                confidences[cat] = 0.0
        
        valid_confs = [c for c in confidences.values() if c > 0]
        overall_confidence = sum(valid_confs) / len(valid_confs) if valid_confs else 0.0
        
        # ê·¼ê±° í’ˆì§ˆ
        total_evidence = sum(
            evaluations[cat]["evidence_count"] 
            for cat in self.CATEGORIES 
            if cat in evaluations
        )
        tier1_evidence = sum(
            evaluations[cat]["tier1_evidence_count"] 
            for cat in self.CATEGORIES 
            if cat in evaluations
        )
        avg_evidence_quality = tier1_evidence / total_evidence if total_evidence > 0 else 0
        
        return {
            **scores,
            "total": total,
            "grade": grade,
            "overall_confidence": round(overall_confidence, 2),
            "confidence_by_category": confidences,
            "average_evidence_quality": round(avg_evidence_quality, 2)
        }
    
    def _identify_critical_issues(self, evaluations: Dict) -> List[Dict]:
        """Critical Issues ì¶”ì¶œ"""
        critical_issues = []
        
        for category, evaluation in evaluations.items():
            if evaluation["score"] < 50:
                for issue in evaluation.get("issues", []):
                    critical_issues.append({
                        "severity": "critical" if evaluation["score"] < 30 else "high",
                        "category": category,
                        "description": issue,
                        "evidence": [e["finding"] for e in evaluation["evidence"][:3]],
                        "recommendation": f"{category} ê°œì„  í•„ìš”",
                        "eu_ai_act_article": None,
                        "source_documents": [e["source_document_id"] for e in evaluation["evidence"]],
                        "evidence_quality": "strong" if evaluation["confidence"] > 0.7 else "moderate",
                        "confidence": evaluation["confidence"]
                    })
        
        return critical_issues
    
    def _check_recollection_needed(self, ethics_score: Dict, evaluations: Dict) -> bool:
        """ì¬ìˆ˜ì§‘ í•„ìš” ì—¬ë¶€"""
        if ethics_score["overall_confidence"] < self.MIN_CONFIDENCE_THRESHOLD:
            print(f"  â†’ ì „ì²´ ì‹ ë¢°ë„ ë¶€ì¡±: {ethics_score['overall_confidence']:.2f} < {self.MIN_CONFIDENCE_THRESHOLD}")
            return True
        
        low_evidence_count = sum(
            1 for cat in self.CATEGORIES
            if cat in evaluations and evaluations[cat]["evidence_count"] < self.MIN_EVIDENCE_COUNT
        )
        
        if low_evidence_count >= 2:
            print(f"  â†’ ê·¼ê±° ë¶€ì¡± ì¹´í…Œê³ ë¦¬: {low_evidence_count}ê°œ")
            return True
        
        return False
    
    def _suggest_collection_focus(self, evaluations: Dict) -> List[str]:
        """ì¬ìˆ˜ì§‘ ì§‘ì¤‘ ì˜ì—­"""
        focus_areas = []
        
        for category, evaluation in evaluations.items():
            if evaluation["confidence"] < 0.5 or evaluation["evidence_count"] < 3:
                for limitation in evaluation.get("limitations", []):
                    focus_areas.append(f"{category}: {limitation}")
        
        return focus_areas[:5]
    
    def _get_empty_evaluation(self) -> EthicsCategoryEvaluation:
        """ë¹ˆ í‰ê°€ ê²°ê³¼"""
        return {
            "score": 0,
            "level": 0,
            "confidence": 0.0,
            "evidence": [],
            "issues": ["í‰ê°€ ì‹¤íŒ¨"],
            "strengths": [],
            "evidence_count": 0,
            "tier1_evidence_count": 0,
            "direct_evidence_count": 0,
            "information_availability": "none",
            "limitations": ["í‰ê°€ ë°ì´í„° ë¶€ì¡±"],
            "criteria_scores": {}
        }