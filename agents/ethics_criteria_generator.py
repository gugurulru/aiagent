# agents/ethics_criteria_generator.py

from typing import Dict, List
from datetime import datetime
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

from state_schema import PipelineState


class EthicsCriteriaGenerator:
    """
    Domain ê¸°ë°˜ ìœ¤ë¦¬ í‰ê°€ ì§€í‘œ ìƒì„± Agent
    
    í”„ë¡œì„¸ìŠ¤:
    1. Domain ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„± (ì˜ˆ: "medical AI transparency requirements")
    2. BM25ë¡œ ê´€ë ¨ EU AI Act ì¡°í•­ 10ê°œ ê²€ìƒ‰
    3. ì¡°í•­ ë¶„ì„í•˜ì—¬ 5ê°œ ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ ê¸°ì¤€ ìƒì„±
    4. ê° ê¸°ì¤€ì— ëŒ€í•œ ìƒì„¸ ê·¼ê±°ì™€ ì°¸ê³  ì¡°í•­ ëª…ì‹œ
    """
    
    # í‰ê°€ ì¹´í…Œê³ ë¦¬
    CATEGORIES = {
        "transparency": "íˆ¬ëª…ì„±",
        "human_oversight": "ì¸ê°„ ê°ë…", 
        "data_governance": "ë°ì´í„° ê±°ë²„ë„ŒìŠ¤",
        "accuracy_validation": "ì •í™•ë„ ê²€ì¦",
        "accountability": "ì±…ì„ì„±"
    }
    
    def __init__(
        self, 
        openai_api_key: str,
        chroma_path: str = "./chroma/ethics",
        collection_name: str = "EU_ai_act"
    ):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            api_key=openai_api_key,
            temperature=0
        )
        
        # ChromaDB ì´ˆê¸°í™”
        embed_fn = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={"trust_remote_code": True},
            encode_kwargs={"normalize_embeddings": True}
        )
        
        self.vectordb = Chroma(
            collection_name=collection_name,
            embedding_function=embed_fn,
            persist_directory=chroma_path
        )
        
        # ëª¨ë“  ë¬¸ì„œ ë¡œë“œ
        print("ğŸ“š EU AI Act ë¬¸ì„œ ë¡œë”© ì¤‘...")
        self.all_docs = self._fetch_all_docs()
        print(f"âœ… {len(self.all_docs)}ê°œ ì¡°í•­ ë¡œë“œ ì™„ë£Œ")
        
        # BM25 Retriever ì´ˆê¸°í™”
        self.bm25 = BM25Retriever.from_documents(self.all_docs)
        self.bm25.k = 10  # ì¹´í…Œê³ ë¦¬ë‹¹ 10ê°œ ì¡°í•­
    
    def execute(self, state: PipelineState) -> PipelineState:
        """ë©”ì¸ ì‹¤í–‰: í‰ê°€ ì§€í‘œ ìƒì„±"""
        print("\n" + "="*70)
        print("ğŸ“‹ [ìœ¤ë¦¬ í‰ê°€ ì§€í‘œ ìƒì„± Agent] ì‹œì‘")
        print("="*70)
        
        domain = state["domain"]
        print(f"ğŸ¥ Domain: {domain}")
        
        try:
            # ê° ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ ì§€í‘œ ìƒì„±
            all_criteria = {}
            
            for category_key, category_name in self.CATEGORIES.items():
                print(f"\n{'='*70}")
                print(f"ğŸ“‹ {category_name} ({category_key}) í‰ê°€ ì§€í‘œ ìƒì„± ì¤‘...")
                print(f"{'='*70}")
                
                # 1) Domain + Category ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„±
                query = self._generate_query(category_key, domain)
                print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
                
                # 2) BM25ë¡œ ê´€ë ¨ ì¡°í•­ 10ê°œ ê²€ìƒ‰
                relevant_articles = self._retrieve_articles(query)
                print(f"ğŸ“œ ê²€ìƒ‰ëœ ì¡°í•­: {len(relevant_articles)}ê°œ")
                
                # ì¡°í•­ ë¯¸ë¦¬ë³´ê¸°
                for i, article in enumerate(relevant_articles[:3], 1):
                    md = article.metadata or {}
                    print(f"   [{i}] Page {md.get('page', 'N/A')}: {article.page_content[:100]}...")
                
                # 3) ì¡°í•­ ê¸°ë°˜ í‰ê°€ ê¸°ì¤€ ìƒì„±
                criteria = self._generate_criteria(
                    category_key=category_key,
                    category_name=category_name,
                    domain=domain,
                    articles=relevant_articles
                )
                
                all_criteria[category_key] = criteria
                
                print(f"\nâœ… {category_name} í‰ê°€ ê¸°ì¤€:")
                print(f"   ìƒì„±ëœ ê¸°ì¤€: {len(criteria['criteria'])}ê°œ")
                for i, crit in enumerate(criteria['criteria'], 1):
                    print(f"   {i}. {crit['title']}")
            
            # State ì—…ë°ì´íŠ¸
            state["ethics_evaluation_criteria"] = {
                "domain": domain,
                "categories": all_criteria,
                "generated_at": datetime.now().isoformat(),
                "total_criteria_count": sum(
                    len(cat['criteria']) for cat in all_criteria.values()
                )
            }
            
            print(f"\n{'='*70}")
            print(f"ğŸ‰ í‰ê°€ ì§€í‘œ ìƒì„± ì™„ë£Œ!")
            print(f"{'='*70}")
            print(f"ğŸ“Š í†µê³„:")
            print(f"   - ì¹´í…Œê³ ë¦¬: {len(all_criteria)}ê°œ")
            print(f"   - ì´ í‰ê°€ ê¸°ì¤€: {state['ethics_evaluation_criteria']['total_criteria_count']}ê°œ")
            print(f"   - Domain: {domain}")
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì§€í‘œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            
            state["errors"].append({
                "stage": "ethics_criteria_generation",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "recovered": False,
                "traceback": traceback.format_exc(),
                "impact_on_reliability": "high"
            })
        
        return state
    
    def _fetch_all_docs(self) -> List[Document]:
        """ChromaDBì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°"""
        col = self.vectordb._collection
        out_docs = []
        total = col.count()
        offset = 0
        batch_size = 1000
        
        while True:
            batch = col.get(
                where={},
                limit=batch_size,
                offset=offset,
                include=["documents", "metadatas"]
            )
            ids = batch.get("ids") or []
            docs = batch.get("documents") or []
            metas = batch.get("metadatas") or []
            
            if not ids:
                break
            
            for txt, md in zip(docs, metas):
                out_docs.append(Document(page_content=txt or "", metadata=md or {}))
            
            offset += len(ids)
            if offset >= total:
                break
        
        return out_docs
    
    def _generate_query(self, category: str, domain: str) -> str:
        """Domain + Category ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        
        # Domainë³„ í‚¤ì›Œë“œ
        domain_keywords = {
            "medical": "healthcare medical diagnosis clinical patient safety",
            "finance": "financial credit risk discrimination fair lending",
            "recruitment": "employment hiring discrimination bias",
            "law_enforcement": "criminal justice predictive policing",
            "education": "student assessment learning profiling"
        }
        
        # Categoryë³„ í‚¤ì›Œë“œ
        category_keywords = {
            "transparency": "transparency explainability disclosure information",
            "human_oversight": "human oversight supervision intervention control",
            "data_governance": "data quality bias training dataset privacy",
            "accuracy_validation": "accuracy performance validation testing evaluation",
            "accountability": "accountability liability responsibility redress"
        }
        
        domain_kw = domain_keywords.get(domain, "AI system")
        category_kw = category_keywords.get(category, "requirements")
        
        # ì¿¼ë¦¬ ìƒì„±
        query = f"EU AI Act {category_kw} requirements for {domain_kw} high-risk AI systems"
        
        return query
    
    def _retrieve_articles(self, query: str) -> List[Document]:
        """BM25ë¡œ ê´€ë ¨ ì¡°í•­ ê²€ìƒ‰"""
        results = self.bm25.get_relevant_documents(query)
        return results[:10]  # ìµœëŒ€ 10ê°œ
    
    def _generate_criteria(
        self,
        category_key: str,
        category_name: str,
        domain: str,
        articles: List[Document]
    ) -> Dict:
        """ì¡°í•­ ê¸°ë°˜ í‰ê°€ ê¸°ì¤€ ìƒì„±"""
        
        # ì¡°í•­ í…ìŠ¤íŠ¸ ì¤€ë¹„
        article_texts = []
        for i, article in enumerate(articles, 1):
            md = article.metadata or {}
            article_texts.append(
                f"[ì¡°í•­ {i}]\n"
                f"ì¶œì²˜: {md.get('source', 'EU AI Act')}\n"
                f"í˜ì´ì§€: {md.get('page', 'N/A')}\n"
                f"ë‚´ìš©:\n{article.page_content}\n"
            )
        
        # í‰ê°€ ê¸°ì¤€ ìƒì„± í”„ë¡¬í”„íŠ¸
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ EU AI Act ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ EU AI Act ì¡°í•­ë“¤ì„ ë¶„ì„í•˜ì—¬ {domain} ë„ë©”ì¸ì˜ AI ì‹œìŠ¤í…œì„ í‰ê°€í•˜ê¸° ìœ„í•œ 
êµ¬ì²´ì ì¸ í‰ê°€ ê¸°ì¤€(criteria)ì„ ìƒì„±í•˜ì„¸ìš”.

ì¤‘ìš” ì§€ì¹¨:
1. ì œê³µëœ ì¡°í•­ì—ì„œë§Œ í‰ê°€ ê¸°ì¤€ì„ ë„ì¶œí•˜ì„¸ìš”
2. ê° ê¸°ì¤€ì€ ì¸¡ì • ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤
3. 5-7ê°œì˜ í•µì‹¬ ê¸°ì¤€ì„ ìƒì„±í•˜ì„¸ìš”
4. ê° ê¸°ì¤€ì— ëŒ€í•´:
   - ì™œ ì´ ê¸°ì¤€ì´ ì¤‘ìš”í•œì§€
   - ì–´ë–¤ ì¡°í•­ì—ì„œ ë„ì¶œë˜ì—ˆëŠ”ì§€
   - êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì„ í‰ê°€í•´ì•¼ í•˜ëŠ”ì§€
   ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”

JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
{{
    "category": "{category_name}",
    "domain": "{domain}",
    "criteria": [
        {{
            "id": "unique_id",
            "title": "í‰ê°€ ê¸°ì¤€ ì œëª© (ê°„ê²°í•˜ê²Œ)",
            "description": "ì´ ê¸°ì¤€ì´ ë¬´ì—‡ì„ í‰ê°€í•˜ëŠ”ì§€ ì„¤ëª… (100ì ì´ë‚´)",
            "importance": "ì™œ ì´ ê¸°ì¤€ì´ {domain} AIì— ì¤‘ìš”í•œì§€ ì„¤ëª… (200ì)",
            "measurement": "êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì„ í™•ì¸í•´ì•¼ í•˜ëŠ”ì§€ (ì˜ˆ: ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€, í”„ë¡œì„¸ìŠ¤ ìœ ë¬´ ë“±)",
            "referenced_articles": [
                {{
                    "article_number": 1,
                    "excerpt": "í•´ë‹¹ ì¡°í•­ì˜ í•µì‹¬ ë¬¸êµ¬ (100ì ì´ë‚´)",
                    "relevance": "ì´ ì¡°í•­ì´ ì™œ ì´ ê¸°ì¤€ì˜ ê·¼ê±°ê°€ ë˜ëŠ”ì§€ ì„¤ëª… (150ì)"
                }}
            ],
            "weight": 0.0-1.0,
            "pass_threshold": "ì´ ê¸°ì¤€ì„ í†µê³¼í•˜ê¸° ìœ„í•œ ìµœì†Œ ìš”êµ¬ì‚¬í•­"
        }}
    ],
    "overall_rationale": "ì´ ì¹´í…Œê³ ë¦¬ì˜ í‰ê°€ ê¸°ì¤€ë“¤ì´ ì™œ {domain} AIì— ì¤‘ìš”í•œì§€ ì¢…í•© ì„¤ëª… (300ì)"
}}"""),
            ("user", """ì¹´í…Œê³ ë¦¬: {category_name} ({category_key})
ë„ë©”ì¸: {domain}

=== EU AI ACT ì¡°í•­ë“¤ ===
{articles}

ìœ„ ì¡°í•­ë“¤ì„ ë¶„ì„í•˜ì—¬ í‰ê°€ ê¸°ì¤€ì„ ìƒì„±í•˜ì„¸ìš”:""")
        ])
        
        chain = prompt | self.llm
        
        response = chain.invoke({
            "category_key": category_key,
            "category_name": category_name,
            "domain": domain,
            "articles": "\n\n".join(article_texts)
        })
        
        # JSON íŒŒì‹±
        try:
            import json
            import re
            
            json_match = re.search(r'```json\s*(.*?)\s*```', response.content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                result = json.loads(response.content)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result["source_articles_count"] = len(articles)
            result["generated_at"] = datetime.now().isoformat()
            
            return result
            
        except Exception as e:
            print(f"    âš ï¸ ê¸°ì¤€ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return {
                "category": category_name,
                "domain": domain,
                "criteria": [],
                "overall_rationale": "ìƒì„± ì‹¤íŒ¨",
                "source_articles_count": 0
            }